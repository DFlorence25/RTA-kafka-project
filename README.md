# ğŸ“Š Real-Time Data Pipeline - Elektronika ZamÃ³wienia

## ğŸ¯ Problem biznesowy

W dobie cyfryzacji i rosnÄ…cych oczekiwaÅ„ klientÃ³w firmy handlowe muszÄ… analizowaÄ‡ dane sprzedaÅ¼owe w czasie rzeczywistym. DziÄ™ki temu mogÄ…:

* ReagowaÄ‡ szybciej na zmiany trendÃ³w zakupowych ğŸ•’
* OptymalizowaÄ‡ stany magazynowe ğŸ“¦
* PersonalizowaÄ‡ oferty i kampanie marketingowe ğŸ

**Naszym celem** byÅ‚o zbudowanie w peÅ‚ni dziaÅ‚ajÄ…cego potoku danych (data pipeline), ktÃ³ry generuje zamÃ³wienia, przesyÅ‚a je przez Apache Kafka, analizuje i wizualizuje na dashboardzie.

---

## ğŸ› ï¸ Co zrobiliÅ›my?

1. **Generator danych** (`data_generator.py`):

   * Tworzy syntetyczne zamÃ³wienia z podziaÅ‚em na regiony, miasta, kanaÅ‚y sprzedaÅ¼y i metody pÅ‚atnoÅ›ci.
   * Wprowadza losowe opÃ³Åºnienia, by symulowaÄ‡ realne obciÄ…Å¼enie.

2. **Producent (Producer)** (`producer.py`):

   * Subskrybuje siÄ™ do generatora i wysyÅ‚a kaÅ¼dÄ… transakcjÄ™ do topiku Kafka `zamowienia_elektronika`.
   * Ustawia klucz partycji na `store_id`, by rozÅ‚oÅ¼yÄ‡ obciÄ…Å¼enie rÃ³wnomiernie.

3. **Konsument (Consumer)** (`consumer.py`):

   * Odbiera wiadomoÅ›ci z Kafki.
   * Zapisuje je do dziennych plikÃ³w CSV.
   * WysyÅ‚a wsady (batch) do Google Sheets, ktÃ³re stanowiÄ… ÅºrÃ³dÅ‚o dla dashboardu Looker Studio.

4. **Dashboard**

   * PodÅ‚Ä…czony do Google Sheets za pomocÄ… Looker Studio.
   * Pokazuje wykresy: wolumen zamÃ³wieÅ„ w czasie, wartoÅ›Ä‡ brutto, podziaÅ‚ na kategorie i regiony.

---

## ğŸš€ Jak uruchomiÄ‡ projekt?

1. **Sklonuj repozytorium**:

   ```bash
   git clone https://github.com/DFlorence25/RTA-kafka-project
   cd RTA-kafka-project
   ```

2. **Dodaj plik poÅ›wiadczeÅ„ Google** (`creds.json`) do katalogu gÅ‚Ã³wnego projektu.

3. **Uruchom z Docker Compose**:

   ```bash
   docker-compose up --build
   ```

   To uruchomi:

   * Zookeeper i Kafka
   * Generator danych
   * Producer
   * Consumer (zapis CSV + Sheets)

4. **OtwÃ³rz dashboard**

   * WejdÅº na Looker Studio i wybierz arkusz `source_looker` jako ÅºrÃ³dÅ‚o danych.

---

## ğŸ“¦ Struktura projektu

```text
real-time-pipeline/
â”œâ”€â”€ data_generator.py   # Skrypt generujÄ…cy zamÃ³wienia
â”œâ”€â”€ producer.py         # WysyÅ‚ka zamÃ³wieÅ„ do Kafki
â”œâ”€â”€ consumer.py         # OdbiÃ³r, CSV + Google Sheets
â”œâ”€â”€ requirements.txt    # ZaleÅ¼noÅ›ci Python
â”œâ”€â”€ docker-compose.yml  # Orkiestracja kontenerÃ³w
â”œâ”€â”€ Dockerfile          # Budowa obrazu Docker
â””â”€â”€ creds.json (âœï¸)     # PoÅ›wiadczenia Google
```

---

## ğŸ“‹ Wymagania

* Docker & Docker Compose
* Konto Google z dostÄ™pem do Google Sheets i wygenerowanym kluczem serwisowym
* Python 3.9+

---

## ğŸ’¡ MoÅ¼liwe rozszerzenia

* Migracja danych z Google Sheets do BigQuery dla wiÄ™kszych wolumenÃ³w ğŸŒ
* Wykorzystanie Avro/Protobuf do wersjonowania schematu wiadomoÅ›ci ğŸ“
* Rozszerzenie generatora o sezonowe i weekendowe wzorce sprzedaÅ¼y ğŸ“ˆ
* Monitorowanie metryk (Prometheus + Grafana) i alerty w przypadku opÃ³ÅºnieÅ„ ğŸš¨

---

<p align="center">Made with â¤ï¸ by ZespÃ³Å‚ Analiza Danych w Czasie Rzeczywistym</p>
